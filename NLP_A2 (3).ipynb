{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NORMALIZATION AND TOKENIZATION**"
      ],
      "metadata": {
        "id": "2JnkL1aH08UP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "lnNyZ2L8hoM8",
        "outputId": "26861166-a3af-4834-dd67-45c6dcd85f8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb75a1bc-8370-46f7-88cc-9f333dcdd547\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cb75a1bc-8370-46f7-88cc-9f333dcdd547\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final_main_dataset.tsv to final_main_dataset (1).tsv\n",
            "Sample sentences after normalization:\n",
            "\n",
            "Original   : کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "Normalized : کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "\n",
            "Original   : اور پھر ممکن ہے کہ پاکستان بھی ہو\n",
            "Normalized : اور پھر ممکن ہی کہ پاکستان بھی ہو\n",
            "\n",
            "Original   : یہ فیصلہ بھی گزشتہ دو سال میں\n",
            "Normalized : یہ فیصلہ بھی گزشتہ دو سال میں\n",
            "\n",
            "Original   : ان کے بلے بازوں کے سامنے ہو گا\n",
            "Normalized : ان کی بلی بازوں کی سامنی ہو گا\n",
            "\n",
            "Original   : آبی جانور میں بطخ بگلا اور دُوسْرا آبی پرندہ شامل ہونا\n",
            "Normalized : ٰابی جانور میں بطخ بگلا اور دوسرا ٰابی پرندہ شامل ہونا\n",
            "\n",
            "\n",
            "Sample tokenized sentences:\n",
            "\n",
            "Normalized : کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "Tokens     : [390, 1933, 17, 3220, 2325, 7927, 3840, 94]\n",
            "\n",
            "Normalized : اور پھر ممکن ہی کہ پاکستان بھی ہو\n",
            "Tokens     : [56, 221, 1282, 17, 68, 125, 77, 24]\n",
            "\n",
            "Normalized : یہ فیصلہ بھی گزشتہ دو سال میں\n",
            "Tokens     : [54, 392, 77, 1884, 105, 355, 27]\n",
            "\n",
            "Normalized : ان کی بلی بازوں کی سامنی ہو گا\n",
            "Tokens     : [51, 11, 1757, 4851, 11, 498, 24, 122]\n",
            "\n",
            "Normalized : ٰابی جانور میں بطخ بگلا اور دوسرا ٰابی پرندہ شامل ہونا\n",
            "Tokens     : [3204, 1619, 27, 7827, 2480, 44, 56, 991, 3204, 2454, 794, 314]\n",
            "\n",
            "Training set size: 16000 (80.0%)\n",
            "Validation set size: 2000 (10.0%)\n",
            "Test set size: 2000 (10.0%)\n",
            "\n",
            "Example tokenized sentences from training set:\n",
            "\n",
            "Sentence: وہ اگر اپنی علاوہ کسی کی بہادری کا ذکر کرتی ہیں۔\n",
            "Tokens  : [81, 312, 121, 1737, 209, 11, 6150, 28, 1310, 222, 86]\n",
            "\n",
            "Sentence: سال میں پہلی بار\n",
            "Tokens  : [355, 27, 290, 344]\n",
            "\n",
            "Sentence: نہ ایک‘ نہ دوسری کی پاس جواب تھا۔\n",
            "Tokens  : [50, 85, 7982, 50, 335, 11, 608, 706, 208]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Full Urdu Text Normalization & Tokenization\n",
        "# ============================================\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Install SentencePiece\n",
        "!pip install sentencepiece --quiet\n",
        "\n",
        "# -----------------------------\n",
        "# Upload Dataset\n",
        "# -----------------------------\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]  # e.g., 'final_main_dataset.tsv'\n",
        "\n",
        "# Load dataset and keep only 'sentence' column\n",
        "df = pd.read_csv(filename, sep='\\t')\n",
        "df = df[['sentence']].dropna()\n",
        "\n",
        "# -----------------------------\n",
        "# Normalization Functions\n",
        "# -----------------------------\n",
        "def remove_diacritics(text):\n",
        "    diacritics = ['َ', 'ً', 'ُ', 'ٌ', 'ِ', 'ٍ', 'ْ', 'ّ', 'ْ', 'ٰ', 'ٔ']\n",
        "    for d in diacritics:\n",
        "        text = text.replace(d, \"\")\n",
        "    return text\n",
        "\n",
        "def standardize_aleph_yeh(text):\n",
        "    text = text.replace('ے','ی' )  # Standardize Yeh\n",
        "    text = text.replace('آ','ٰا' )  # Standardize Alef\n",
        "    return text\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = remove_diacritics(text)\n",
        "    text = standardize_aleph_yeh(text)\n",
        "    return text\n",
        "\n",
        "#  Apply Normalization\n",
        "# -----------------------------\n",
        "df['normalized'] = df['sentence'].apply(normalize_text)\n",
        "\n",
        "# Display first 5 sentences with original and normalized form\n",
        "print(\"Sample sentences after normalization:\\n\")\n",
        "for i in range(min(5, len(df))):\n",
        "    print(f\"Original   : {df['sentence'].iloc[i]}\")\n",
        "    print(f\"Normalized : {df['normalized'].iloc[i]}\\n\")\n",
        "\n",
        "\n",
        "#Save Normalized Sentences for SentencePiece\n",
        "# -----------------------------\n",
        "df['normalized'].to_csv(\"corpus.txt\", index=False, header=False)\n",
        "\n",
        "# -----------------------------\n",
        "#  Train SentencePiece Tokenizer\n",
        "# -----------------------------\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"corpus.txt\",\n",
        "    model_prefix=\"ur_model\",\n",
        "    vocab_size=8000,\n",
        "    character_coverage=0.9995,\n",
        "    model_type='bpe',\n",
        "    user_defined_symbols='[MASK],<sos>,<eos>,<pad>'\n",
        ")\n",
        "\n",
        "# Load trained model\n",
        "sp = spm.SentencePieceProcessor(model_file='ur_model.model')\n",
        "\n",
        "# -----------------------------\n",
        "#  Tokenize Sentences\n",
        "# -----------------------------\n",
        "df['tokenized'] = df['normalized'].apply(lambda x: sp.encode(x, out_type=int))\n",
        "\n",
        "# Display first 5 sentences with normalized text and tokenized form\n",
        "print(\"\\nSample tokenized sentences:\\n\")\n",
        "for i in range(min(5, len(df))):\n",
        "    print(f\"Normalized : {df['normalized'].iloc[i]}\")\n",
        "    print(f\"Tokens     : {df['tokenized'].iloc[i]}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Split Dataset\n",
        "# -----------------------------\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print split sizes\n",
        "print(f\"Training set size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Validation set size: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test set size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Save splits (optional)\n",
        "train_df.to_csv(\"train.csv\", index=False)\n",
        "val_df.to_csv(\"val.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "#  Example of tokenized output\n",
        "# -----------------------------\n",
        "print(\"\\nExample tokenized sentences from training set:\\n\")\n",
        "for i in range(min(3, len(train_df))):\n",
        "    print(f\"Sentence: {train_df['normalized'].iloc[i]}\")\n",
        "    print(f\"Tokens  : {train_df['tokenized'].iloc[i]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ARCHITECTURE: TRANSFORMER**"
      ],
      "metadata": {
        "id": "61i6XyC913Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_v = d_model // num_heads\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        Q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "        K = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "        V = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "        att_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_v)\n",
        "        if mask is not None:\n",
        "            att_scores = att_scores.masked_fill(mask == 0, -1e9)\n",
        "        att_probs = F.softmax(att_scores, dim=-1)\n",
        "        att_output = torch.matmul(att_probs, V).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.w_o(att_output)\n",
        "\n",
        "# Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=1024):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(F.relu(self.lin1(x)))\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.dropout(self.attn(x, x, x, mask))\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.dropout(self.ff(x))\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        self_attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.norm1(x + self_attn_output)\n",
        "        enc_dec_attn_output = self.dropout(self.enc_dec_attn(x, enc_output, enc_output, src_mask))\n",
        "        x = self.norm2(x + enc_dec_attn_output)\n",
        "        ff_output = self.dropout(self.ff(x))\n",
        "        x = self.norm3(x + ff_output)\n",
        "        return x\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.pe(self.embed(src))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_output, src_mask, tgt_mask):\n",
        "        x = self.pe(self.embed(tgt))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_layers=2, num_heads=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size, d_model, num_layers, num_heads, dropout)\n",
        "        self.decoder = Decoder(vocab_size, d_model, num_layers, num_heads, dropout)\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        return self.linear(dec_output)\n",
        "\n",
        "# Example instantiation\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = 8000  # Example vocab size from SentencePiece\n",
        "    model = Transformer(vocab_size=vocab_size, d_model=256, num_layers=2, num_heads=2, dropout=0.1)\n",
        "    print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7ncDldQi36t",
        "outputId": "1d713b25-65fb-4326-f482-20c3b090d207"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Embedding(8000, 256)\n",
            "    (pe): PositionalEncoding()\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x EncoderLayer(\n",
            "        (attn): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (ff): FeedForward(\n",
            "          (lin1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (lin2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embed): Embedding(8000, 256)\n",
            "    (pe): PositionalEncoding()\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x DecoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (enc_dec_attn): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (ff): FeedForward(\n",
            "          (lin1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (lin2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (linear): Linear(in_features=256, out_features=8000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLr_1u-CjmIG",
        "outputId": "95c76a70-d80f-4515-b8f4-2ff9f8b411aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING AND PARAMETERS**"
      ],
      "metadata": {
        "id": "lrdvm_rw2MTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Dataset Class\n",
        "# ============================================\n",
        "class UrduDataset(Dataset):\n",
        "    def __init__(self, df, pad_id=0, max_len=100):\n",
        "        # Convert string representation of list back to list if needed\n",
        "        if isinstance(df['tokenized'].iloc[0], str):\n",
        "            self.sentences = df['tokenized'].apply(ast.literal_eval).tolist()\n",
        "        else:\n",
        "            self.sentences = df['tokenized'].tolist()\n",
        "        self.pad_id = pad_id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.sentences[idx][:self.max_len - 2]  # Reserve for <sos>, <eos>\n",
        "        src = [1] + tokens + [2]  # <sos>=1, <eos>=2\n",
        "        tgt = [1] + tokens + [2]\n",
        "\n",
        "        # Pad sequences\n",
        "        src += [self.pad_id] * (self.max_len - len(src))\n",
        "        tgt += [self.pad_id] * (self.max_len - len(tgt))\n",
        "\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "# ============================================\n",
        "# Masking Function\n",
        "# ============================================\n",
        "def create_mask(src, tgt, pad_id=0):\n",
        "    # src: (B, src_len), tgt: (B, tgt_len)\n",
        "    src_mask = (src != pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,src_len)\n",
        "    tgt_pad_mask = (tgt != pad_id).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "    tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Translation Function\n",
        "# ============================================\n",
        "def translate_sentence(model, sentence, sp, device='cuda' if torch.cuda.is_available() else 'cpu', max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input sentence\n",
        "        tokens = sp.encode(sentence, out_type=int)\n",
        "        src = [1] + tokens + [2]  # Add <sos> and <eos>\n",
        "        src = torch.tensor(src).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
        "\n",
        "        # Create source mask\n",
        "        src_mask, _ = create_mask(src, src)\n",
        "\n",
        "        # Encode the source sentence\n",
        "        enc_out = model.encoder(src, src_mask)\n",
        "\n",
        "        # Initialize decoded sequence with <sos>\n",
        "        decoded = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "        # Decode token by token\n",
        "        for _ in range(max_len):\n",
        "            _, tgt_mask = create_mask(src, decoded)\n",
        "            out = model.decoder(decoded, enc_out, src_mask, tgt_mask)\n",
        "            logits = model.linear(out[:, -1, :])\n",
        "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            decoded = torch.cat((decoded, next_token), dim=1)\n",
        "\n",
        "            if next_token.item() == 2:  # Stop if <eos> is predicted\n",
        "                break\n",
        "\n",
        "        # Decode the tokenized output back to text\n",
        "        translation = sp.decode([t for t in decoded.squeeze(0).tolist() if t not in [0, 1, 2]]) # remove pad, sos, eos\n",
        "        return translation\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Training Function\n",
        "# ============================================\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        loss = criterion(logits, tgt_output)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# ============================================\n",
        "# BLEU Evaluation\n",
        "# ============================================\n",
        "def evaluate_bleu(model, dataloader, sp, device, max_len=50):\n",
        "    model.eval()\n",
        "    references, hypotheses = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            src = src.to(device)\n",
        "            src_mask, _ = create_mask(src, src)\n",
        "            enc_out = model.encoder(src, src_mask)\n",
        "\n",
        "            decoded = torch.ones((src.size(0), 1), dtype=torch.long, device=device)  # start with <sos>\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                _, tgt_mask = create_mask(src, decoded)\n",
        "                out = model.decoder(decoded, enc_out, src_mask, tgt_mask)\n",
        "                logits = model.linear(out[:, -1, :])\n",
        "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "                decoded = torch.cat((decoded, next_token), dim=1)\n",
        "                if (next_token == 2).all():  # stop if all finished\n",
        "                    break\n",
        "\n",
        "            for ref, hyp in zip(tgt.cpu().tolist(), decoded.cpu().tolist()):\n",
        "                ref_text = sp.decode([t for t in ref if t not in [0, 1, 2]])  # remove pad, sos, eos\n",
        "                hyp_text = sp.decode([t for t in hyp if t not in [0, 1, 2]])\n",
        "                references.append([ref_text])\n",
        "                hypotheses.append(hyp_text)\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, list(zip(*references)))\n",
        "    return bleu.score\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Main Training Script\n",
        "# ============================================\n",
        "def main():\n",
        "\n",
        "\n",
        "    # Positional Encoding\n",
        "    class PositionalEncoding(nn.Module):\n",
        "        def __init__(self, d_model, max_len=5000):\n",
        "            super().__init__()\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "    # Multi-Head Attention\n",
        "    class MultiHeadAttention(nn.Module):\n",
        "        def __init__(self, d_model, num_heads):\n",
        "            super().__init__()\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "            self.num_heads = num_heads\n",
        "            self.d_model = d_model\n",
        "            self.d_v = d_model // num_heads\n",
        "            self.w_q = nn.Linear(d_model, d_model)\n",
        "            self.w_k = nn.Linear(d_model, d_model)\n",
        "            self.w_v = nn.Linear(d_model, d_model)\n",
        "            self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        def forward(self, q, k, v, mask=None):\n",
        "            batch_size = q.size(0)\n",
        "            Q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "            K = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "            V = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "            att_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_v)\n",
        "            if mask is not None:\n",
        "                att_scores = att_scores.masked_fill(mask == 0, -1e9)\n",
        "            att_probs = F.softmax(att_scores, dim=-1)\n",
        "            att_output = torch.matmul(att_probs, V).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "            return self.w_o(att_output)\n",
        "\n",
        "    # Feed-Forward Network\n",
        "    class FeedForward(nn.Module):\n",
        "        def __init__(self, d_model, d_ff=1024):\n",
        "            super().__init__()\n",
        "            self.lin1 = nn.Linear(d_model, d_ff)\n",
        "            self.lin2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.lin2(F.relu(self.lin1(x)))\n",
        "\n",
        "    # Encoder Layer\n",
        "    class EncoderLayer(nn.Module):\n",
        "        def __init__(self, d_model, num_heads, dropout=0.3):\n",
        "            super().__init__()\n",
        "            self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "            self.ff = FeedForward(d_model)\n",
        "            self.norm1 = nn.LayerNorm(d_model)\n",
        "            self.norm2 = nn.LayerNorm(d_model)\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def forward(self, x, mask):\n",
        "            attn_output = self.dropout(self.attn(x, x, x, mask))\n",
        "            x = self.norm1(x + attn_output)\n",
        "            ff_output = self.dropout(self.ff(x))\n",
        "            x = self.norm2(x + ff_output)\n",
        "            return x\n",
        "\n",
        "    # Decoder Layer\n",
        "    class DecoderLayer(nn.Module):\n",
        "        def __init__(self, d_model, num_heads, dropout=0.3):\n",
        "            super().__init__()\n",
        "            self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "            self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "            self.ff = FeedForward(d_model)\n",
        "            self.norm1 = nn.LayerNorm(d_model)\n",
        "            self.norm2 = nn.LayerNorm(d_model)\n",
        "            self.norm3 = nn.LayerNorm(d_model)\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "            self_attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n",
        "            x = self.norm1(x + self_attn_output)\n",
        "            enc_dec_attn_output = self.dropout(self.enc_dec_attn(x, enc_output, enc_output, src_mask))\n",
        "            x = self.norm2(x + enc_dec_attn_output)\n",
        "            ff_output = self.dropout(self.ff(x))\n",
        "            x = self.norm3(x + ff_output)\n",
        "            return x\n",
        "\n",
        "    # Encoder\n",
        "    class Encoder(nn.Module):\n",
        "        def __init__(self, vocab_size, d_model, num_layers, num_heads, dropout):\n",
        "            super().__init__()\n",
        "            self.embed = nn.Embedding(vocab_size, d_model)\n",
        "            self.pe = PositionalEncoding(d_model)\n",
        "            self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "            self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        def forward(self, src, mask):\n",
        "            x = self.pe(self.embed(src))\n",
        "            for layer in self.layers:\n",
        "                x = layer(x, mask)\n",
        "            return self.norm(x)\n",
        "\n",
        "    # Decoder\n",
        "    class Decoder(nn.Module):\n",
        "        def __init__(self, vocab_size, d_model, num_layers, num_heads, dropout):\n",
        "            super().__init__()\n",
        "            self.embed = nn.Embedding(vocab_size, d_model)\n",
        "            self.pe = PositionalEncoding(d_model)\n",
        "            self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "            self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        def forward(self, tgt, enc_output, src_mask, tgt_mask):\n",
        "            x = self.pe(self.embed(tgt))\n",
        "            for layer in self.layers:\n",
        "                x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "            return self.norm(x)\n",
        "\n",
        "    # Transformer Model\n",
        "    class Transformer(nn.Module):\n",
        "        def __init__(self, vocab_size, d_model=256, num_layers=2, num_heads=2, dropout=0.1):\n",
        "            super().__init__()\n",
        "            self.encoder = Encoder(vocab_size, d_model, num_layers, num_heads, dropout)\n",
        "            self.decoder = Decoder(vocab_size, d_model, num_layers, num_heads, dropout)\n",
        "            self.linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "            enc_output = self.encoder(src, src_mask)\n",
        "            dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "            return self.linear(dec_output)\n",
        "\n",
        "\n",
        "    # ---- Load Data ----\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    val_df = pd.read_csv(\"val.csv\")\n",
        "\n",
        "    # ---- Dataset & Dataloader ----\n",
        "    train_dataset = UrduDataset(train_df, pad_id=0)\n",
        "    val_dataset = UrduDataset(val_df, pad_id=0)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    # ---- Load SentencePiece ----\n",
        "    import sentencepiece as spm\n",
        "    sp = spm.SentencePieceProcessor(model_file='ur_model.model')\n",
        "\n",
        "    # ---- Model, Optimizer, Loss ----\n",
        "    vocab_size = sp.get_piece_size()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Transformer(vocab_size=vocab_size, d_model=256, num_layers=2, num_heads=2, dropout=0.1)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    best_bleu = 0.0\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_bleu = evaluate_bleu(model, val_loader, sp, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Validation BLEU: {val_bleu:.2f}\")\n",
        "\n",
        "        if val_bleu > best_bleu:\n",
        "            best_bleu = val_bleu\n",
        "            torch.save(model.state_dict(), \"best_transformer.pt\")\n",
        "            print(f\"✅ Saved new best model with BLEU = {best_bleu:.2f}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Example sentences to test\n",
        "    # -----------------------------\n",
        "    test_sentences = [\n",
        "        \"میں لاہور گیا تھا۔\",\n",
        "        \"آج موسم بہت اچھا ہے۔\",\n",
        "        \"آپ کیسے ہیں؟\",\n",
        "        \"کل اسکول جانا ہے۔\",\n",
        "        \"یہ ایک بہت خوبصورت جگہ ہے۔\"\n",
        "    ]\n",
        "\n",
        "    # Apply normalization first\n",
        "    def normalize_text(text):\n",
        "        diacritics = ['َ', 'ً', 'ُ', 'ٌ', 'ِ', 'ٍ', 'ْ', 'ّ', 'ْ', 'ٰ', 'ٔ']\n",
        "        for d in diacritics:\n",
        "            text = text.replace(d, \"\")\n",
        "        text = text.replace('ے','ی' )  # Standardize Yeh\n",
        "        text = text.replace('آ','ٰا' )  # Standardize Alef (corrected from 'آ' -> 'ٰا' to 'ٰا' -> 'آ')\n",
        "        return text\n",
        "    normalized_sentences = [normalize_text(s) for s in test_sentences]\n",
        "\n",
        "    # Display normalized sentences\n",
        "    print(\"Normalized Sentences:\\n\")\n",
        "    for orig, norm in zip(test_sentences, normalized_sentences):\n",
        "        print(f\"Original   : {orig}\")\n",
        "        print(f\"Normalized : {norm}\\n\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Translate using trained model\n",
        "    # -----------------------------\n",
        "    print(\"\\nTranslated Sentences:\\n\")\n",
        "    for sentence in normalized_sentences:\n",
        "        translation = translate_sentence(model, sentence, sp)\n",
        "        print(f\"Input : {sentence}\")\n",
        "        print(f\"Output: {translation}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc7uUDLIi_1Q",
        "outputId": "b214e74d-119f-42fb-ddc1-56feb0efb28c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 6.1356 | Validation BLEU: 3.96\n",
            "✅ Saved new best model with BLEU = 3.96\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.1912 | Validation BLEU: 18.49\n",
            "✅ Saved new best model with BLEU = 18.49\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.9594 | Validation BLEU: 33.85\n",
            "✅ Saved new best model with BLEU = 33.85\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0958 | Validation BLEU: 49.18\n",
            "✅ Saved new best model with BLEU = 49.18\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.4905 | Validation BLEU: 59.77\n",
            "✅ Saved new best model with BLEU = 59.77\n",
            "Normalized Sentences:\n",
            "\n",
            "Original   : میں لاہور گیا تھا۔\n",
            "Normalized : میں لاہور گیا تھا۔\n",
            "\n",
            "Original   : آج موسم بہت اچھا ہے۔\n",
            "Normalized : ٰاج موسم بہت اچھا ہی۔\n",
            "\n",
            "Original   : آپ کیسے ہیں؟\n",
            "Normalized : ٰاپ کیسی ہیں؟\n",
            "\n",
            "Original   : کل اسکول جانا ہے۔\n",
            "Normalized : کل اسکول جانا ہی۔\n",
            "\n",
            "Original   : یہ ایک بہت خوبصورت جگہ ہے۔\n",
            "Normalized : یہ ایک بہت خوبصورت جگہ ہی۔\n",
            "\n",
            "\n",
            "Translated Sentences:\n",
            "\n",
            "Input : میں لاہور گیا تھا۔\n",
            "Output: میں لاہور گیا تھا۔\n",
            "\n",
            "Input : ٰاج موسم بہت اچھا ہی۔\n",
            "Output: ٰاج موسم بہت اچھا ہی۔\n",
            "\n",
            "Input : ٰاپ کیسی ہیں؟\n",
            "Output: ٰاپ کیسی ہیں؟\n",
            "\n",
            "Input : کل اسکول جانا ہی۔\n",
            "Output: کل فلم جانا ہی۔\n",
            "\n",
            "Input : یہ ایک بہت خوبصورت جگہ ہی۔\n",
            "Output: یہ ایک بہت خوبصورت جگہ ہی۔\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import math\n",
        "import sentencepiece as spm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================\n",
        "# Dataset Class\n",
        "# ============================================\n",
        "class UrduConversationDataset(Dataset):\n",
        "    def __init__(self, df, pad_id=0, max_len=50):\n",
        "        self.sentences = df['sentence'].tolist()\n",
        "        self.pad_id = pad_id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(1, len(self.sentences) - 1)  # Ensure at least 1 pair\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.sentences[idx]\n",
        "        tgt_text = self.sentences[min(idx + 1, len(self.sentences) - 1)]  # Avoid index out of range\n",
        "        return src_text, tgt_text\n",
        "\n",
        "# ============================================\n",
        "# Masking Function\n",
        "# ============================================\n",
        "def create_mask(src, tgt, pad_id=0):\n",
        "    src_mask = (src != pad_id).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_pad_mask = (tgt != pad_id).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "    tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# ============================================\n",
        "# Translation Function\n",
        "# ============================================\n",
        "def translate_sentence(model, sentence, sp, device='cuda' if torch.cuda.is_available() else 'cpu', max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            tokens = sp.encode(sentence, out_type=int)\n",
        "            src = torch.tensor([1] + tokens[:max_len - 2] + [2]).unsqueeze(0).to(device)\n",
        "            src_mask, _ = create_mask(src, src)\n",
        "            enc_out = model.encoder(src, src_mask)\n",
        "            decoded = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "            for _ in range(max_len):\n",
        "                _, tgt_mask = create_mask(src, decoded)\n",
        "                out = model.decoder(decoded, enc_out, src_mask, tgt_mask)\n",
        "                logits = model.linear(out[:, -1, :])\n",
        "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "                decoded = torch.cat((decoded, next_token), dim=1)\n",
        "                if next_token.item() == 2:\n",
        "                    break\n",
        "            translation = sp.decode([t for t in decoded.squeeze(0).tolist() if t not in [0, 1, 2]])\n",
        "            return translation\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error for '{sentence}': {e}\")\n",
        "            return \"\"\n",
        "\n",
        "# ============================================\n",
        "# Training Function\n",
        "# ============================================\n",
        "def train_epoch(model, dataloader, optimizer, criterion, sp, device, max_len=50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src_text, tgt_text in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        try:\n",
        "            src_tokens = [sp.encode(s, out_type=int)[:max_len - 2] for s in src_text]\n",
        "            tgt_tokens = [sp.encode(t, out_type=int)[:max_len - 2] for t in tgt_text]\n",
        "            if not src_tokens or not tgt_tokens:\n",
        "                continue  # Skip empty batches\n",
        "            src = [torch.tensor([1] + t + [2]) for t in src_tokens]\n",
        "            tgt = [torch.tensor([1] + t + [2]) for t in tgt_tokens]\n",
        "            src = torch.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0).to(device)\n",
        "            tgt = torch.nn.utils.rnn.pad_sequence(tgt, batch_first=True, padding_value=0).to(device)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(src, tgt_input, src_mask, tgt_mask)\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "            loss = criterion(logits, tgt_output)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        except Exception as e:\n",
        "            print(f\"Training batch error: {e}\")\n",
        "            continue\n",
        "    return total_loss / max(1, len(dataloader))\n",
        "\n",
        "# ============================================\n",
        "# BLEU Evaluation\n",
        "# ============================================\n",
        "def evaluate_bleu(model, dataloader, sp, device, max_len=50):\n",
        "    model.eval()\n",
        "    references, hypotheses = [], []\n",
        "    with torch.no_grad():\n",
        "        for src_text, tgt_text in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            try:\n",
        "                src_tokens = [sp.encode(s, out_type=int)[:max_len - 2] for s in src_text]\n",
        "                src = torch.nn.utils.rnn.pad_sequence(\n",
        "                    [torch.tensor([1] + t + [2]) for t in src_tokens], batch_first=True, padding_value=0\n",
        "                ).to(device)\n",
        "                src_mask, _ = create_mask(src, src)\n",
        "                enc_out = model.encoder(src, src_mask)\n",
        "                decoded = torch.ones((src.size(0), 1), dtype=torch.long, device=device)\n",
        "                for _ in range(max_len):\n",
        "                    _, tgt_mask = create_mask(src, decoded)\n",
        "                    out = model.decoder(decoded, enc_out, src_mask, tgt_mask)\n",
        "                    logits = model.linear(out[:, -1, :])\n",
        "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "                    decoded = torch.cat((decoded, next_token), dim=1)\n",
        "                    if (next_token == 2).all():\n",
        "                        break\n",
        "                for ref, hyp in zip(tgt_text, decoded.cpu().tolist()):\n",
        "                    ref_text = sp.decode([t for t in sp.encode(ref, out_type=int) if t not in [0, 1, 2]])\n",
        "                    hyp_text = sp.decode([t for t in hyp if t not in [0, 1, 2]])\n",
        "                    references.append([ref_text])\n",
        "                    hypotheses.append(hyp_text)\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation error: {e}\")\n",
        "                continue\n",
        "    try:\n",
        "        bleu = sacrebleu.corpus_bleu(hypotheses, list(zip(*references)))\n",
        "        return bleu.score\n",
        "    except Exception as e:\n",
        "        print(f\"BLEU calculation error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# ============================================\n",
        "# Model Architecture\n",
        "# ============================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_v = d_model // num_heads\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        Q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "        K = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "        V = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "        att_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_v)\n",
        "        if mask is not None:\n",
        "            att_scores = att_scores.masked_fill(mask == 0, -1e9)\n",
        "        att_probs = F.softmax(att_scores, dim=-1)\n",
        "        att_output = torch.matmul(att_probs, V).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.w_o(att_output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=1024):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(F.relu(self.lin1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.dropout(self.attn(x, x, x, mask))\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.dropout(self.ff(x))\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        self_attn_output = self.dropout(self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.norm1(x + self_attn_output)\n",
        "        enc_dec_attn_output = self.dropout(self.enc_dec_attn(x, enc_output, enc_output, src_mask))\n",
        "        x = self.norm2(x + enc_dec_attn_output)\n",
        "        ff_output = self.dropout(self.ff(x))\n",
        "        x = self.norm3(x + ff_output)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.pe(self.embed(src))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_output, src_mask, tgt_mask):\n",
        "        x = self.pe(self.embed(tgt))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_layers=2, num_heads=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size, d_model, num_layers, num_heads, dropout)\n",
        "        self.decoder = Decoder(vocab_size, d_model, num_layers, num_heads, dropout)\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        return self.linear(dec_output)\n",
        "\n",
        "# ============================================\n",
        "# Main Training Script\n",
        "# ============================================\n",
        "def main():\n",
        "    # ---- Normalize Text ----\n",
        "    def normalize_text(text):\n",
        "        diacritics = ['َ', 'ً', 'ُ', 'ٌ', 'ِ', 'ٍ', 'ْ', 'ّ', 'ْ', 'ٰ', 'ٔ']\n",
        "        for d in diacritics:\n",
        "            text = text.replace(d, \"\")\n",
        "        text = text.replace('ے', 'ی')  # Standardize Yeh\n",
        "        text = text.replace('آ', 'ٰا')  # Standardize Alef\n",
        "        return text\n",
        "\n",
        "    # ---- Load and Preprocess Data ----\n",
        "    try:\n",
        "        df = pd.read_csv(\"final_main_dataset.tsv\", sep='\\t')\n",
        "        print(f\"Loaded dataset with {len(df)} rows\")\n",
        "        print(df.head())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    df['sentence'] = df['sentence'].apply(normalize_text)\n",
        "    train_size = int(0.8 * len(df))\n",
        "    val_size = int(0.1 * len(df))\n",
        "    train_df = df[:train_size]\n",
        "    val_df = df[train_size:train_size + val_size]\n",
        "\n",
        "    # ---- Load SentencePiece ----\n",
        "    try:\n",
        "        sp = spm.SentencePieceProcessor(model_file='ur_model.model')\n",
        "        print(f\"SentencePiece vocab size: {sp.get_piece_size()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading SentencePiece model: {e}\")\n",
        "        return\n",
        "\n",
        "    # ---- Dataset & Dataloader ----\n",
        "    train_dataset = UrduConversationDataset(train_df, pad_id=0, max_len=50)\n",
        "    val_dataset = UrduConversationDataset(val_df, pad_id=0, max_len=50)\n",
        "    print(f\"Train dataset size: {len(train_dataset)}, Val dataset size: {len(val_dataset)}\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    # ---- Model, Optimizer, Loss ----\n",
        "    vocab_size = sp.get_piece_size()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Transformer(vocab_size=vocab_size, d_model=256, num_layers=2, num_heads=2, dropout=0.1)\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    best_bleu = 0.0\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, sp, device)\n",
        "        val_bleu = evaluate_bleu(model, val_loader, sp, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Validation BLEU: {val_bleu:.2f}\")\n",
        "        if val_bleu > best_bleu:\n",
        "            best_bleu = val_bleu\n",
        "            torch.save(model.state_dict(), \"best_transformer.pt\")\n",
        "            print(f\"✅ Saved new best model with BLEU = {best_bleu:.2f}\")\n",
        "\n",
        "    # ---- Test Sentences ----\n",
        "    test_sentences = [\n",
        "        \"میں لاہور گیا تھا۔\",\n",
        "        \"آج موسم بہت اچھا ہے۔\",\n",
        "        \"آپ کیسے ہیں؟\",\n",
        "        \"کل اسکول جانا ہے۔\",\n",
        "        \"یہ ایک بہت خوبصورت جگہ ہے۔\"\n",
        "    ]\n",
        "    normalized_sentences = [normalize_text(s) for s in test_sentences]\n",
        "\n",
        "    print(\"\\nNormalized Sentences:\\n\")\n",
        "    for orig, norm in zip(test_sentences, normalized_sentences):\n",
        "        print(f\"Original   : {orig}\")\n",
        "        print(f\"Normalized : {norm}\\n\")\n",
        "\n",
        "    print(\"\\nGenerated Responses:\\n\")\n",
        "    for sentence in normalized_sentences:\n",
        "        response = translate_sentence(model, sentence, sp, device)\n",
        "        print(f\"Input : {sentence}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ8lVA-gx2jg",
        "outputId": "84c41381-78f1-44de-ed37-e809420b8bde"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 20000 rows\n",
            "                                           client_id  \\\n",
            "0  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "1  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "2  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "3  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "4  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "\n",
            "                           path  \\\n",
            "0  common_voice_ur_31771683.mp3   \n",
            "1  common_voice_ur_31771684.mp3   \n",
            "2  common_voice_ur_31771685.mp3   \n",
            "3  common_voice_ur_31771730.mp3   \n",
            "4  common_voice_ur_31771732.mp3   \n",
            "\n",
            "                                            sentence  up_votes  down_votes  \\\n",
            "0                 کبھی کبھار ہی خیالی پلاو بناتا ہوں         2           0   \n",
            "1                  اور پھر ممکن ہے کہ پاکستان بھی ہو         2           1   \n",
            "2                      یہ فیصلہ بھی گزشتہ دو سال میں         2           0   \n",
            "3                     ان کے بلے بازوں کے سامنے ہو گا         3           0   \n",
            "4  آبی جانور میں بطخ بگلا اور دُوسْرا آبی پرندہ ش...         3           0   \n",
            "\n",
            "        age gender accents  variant locale  segment  \n",
            "0  twenties   male     NaN      NaN     ur      NaN  \n",
            "1  twenties   male     NaN      NaN     ur      NaN  \n",
            "2  twenties   male     NaN      NaN     ur      NaN  \n",
            "3  twenties   male     NaN      NaN     ur      NaN  \n",
            "4  twenties   male     NaN      NaN     ur      NaN  \n",
            "SentencePiece vocab size: 8000\n",
            "Train dataset size: 15999, Val dataset size: 1999\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 6.5414 | Validation BLEU: 0.17\n",
            "✅ Saved new best model with BLEU = 0.17\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.8890 | Validation BLEU: 0.05\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.4677 | Validation BLEU: 0.12\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.0900 | Validation BLEU: 0.14\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.7265 | Validation BLEU: 0.10\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.3646 | Validation BLEU: 0.09\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.0035 | Validation BLEU: 0.12\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6446 | Validation BLEU: 0.17\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.3014 | Validation BLEU: 0.17\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.9678 | Validation BLEU: 0.22\n",
            "✅ Saved new best model with BLEU = 0.22\n",
            "\n",
            "Normalized Sentences:\n",
            "\n",
            "Original   : میں لاہور گیا تھا۔\n",
            "Normalized : میں لاہور گیا تھا۔\n",
            "\n",
            "Original   : آج موسم بہت اچھا ہے۔\n",
            "Normalized : ٰاج موسم بہت اچھا ہی۔\n",
            "\n",
            "Original   : آپ کیسے ہیں؟\n",
            "Normalized : ٰاپ کیسی ہیں؟\n",
            "\n",
            "Original   : کل اسکول جانا ہے۔\n",
            "Normalized : کل اسکول جانا ہی۔\n",
            "\n",
            "Original   : یہ ایک بہت خوبصورت جگہ ہے۔\n",
            "Normalized : یہ ایک بہت خوبصورت جگہ ہی۔\n",
            "\n",
            "\n",
            "Generated Responses:\n",
            "\n",
            "Input : میں لاہور گیا تھا۔\n",
            "Response: میں نی اپنی سی ایک کو مسترد کر بریک لگائی ہوئی ہی\n",
            "\n",
            "Input : ٰاج موسم بہت اچھا ہی۔\n",
            "Response: یہ ایک اچھی پلئیرز دیی ہیں۔\n",
            "\n",
            "Input : ٰاپ کیسی ہیں؟\n",
            "Response: لیکن اب سنا ہی۔\n",
            "\n",
            "Input : کل اسکول جانا ہی۔\n",
            "Response: وہ بھی نہ تھی۔\n",
            "\n",
            "Input : یہ ایک بہت خوبصورت جگہ ہی۔\n",
            "Response: اب تو ہم کیوں نہیں کر سکتی؟\n",
            "\n"
          ]
        }
      ]
    }
  ]
}